{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Objectives\" data-toc-modified-id=\"Objectives-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Objectives</a></span></li><li><span><a href=\"#Solving-the-Line-of-Best-Fit-by-Guessing\" data-toc-modified-id=\"Solving-the-Line-of-Best-Fit-by-Guessing-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Solving the Line of Best Fit by Guessing</a></span></li><li><span><a href=\"#The-Loss-Function\" data-toc-modified-id=\"The-Loss-Function-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>The Loss Function</a></span></li><li><span><a href=\"#The-Cost-Function\" data-toc-modified-id=\"The-Cost-Function-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>The Cost Function</a></span></li><li><span><a href=\"#Better-Way-of-Guessing:-Gradient-Descent\" data-toc-modified-id=\"Better-Way-of-Guessing:-Gradient-Descent-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Better Way of Guessing: Gradient Descent</a></span><ul class=\"toc-item\"><li><span><a href=\"#Gradient-Descent-in-Words\" data-toc-modified-id=\"Gradient-Descent-in-Words-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Gradient Descent in Words</a></span></li><li><span><a href=\"#Stepping-Down-a-Hill:-Step-Size\" data-toc-modified-id=\"Stepping-Down-a-Hill:-Step-Size-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Stepping Down a Hill: Step Size</a></span></li><li><span><a href=\"#Putting-It-All-Together\" data-toc-modified-id=\"Putting-It-All-Together-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Putting It All Together</a></span></li></ul></li><li><span><a href=\"#Gradient-Descent-Walkthrough\" data-toc-modified-id=\"Gradient-Descent-Walkthrough-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Gradient Descent Walkthrough</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explain and use the concept of a gradient\n",
    "- Explain the algorithm of gradient descent\n",
    "- Describe the effect of the \"learning rate\" in the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving the Line of Best Fit by Guessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we have some data below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly created data in x & y\n",
    "np.random.seed(42)\n",
    "\n",
    "x = np.random.rand(30, 1).reshape(30)\n",
    "y_randterm = np.random.normal(0, 3, 30)\n",
    "y = 3 + 50 * x + y_randterm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the data plotted out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(x, y)\n",
    "\n",
    "ax.set_title('Data Points to Model')\n",
    "ax.set_xlabel('x', fontsize=14)\n",
    "ax.set_ylabel('y', fontsize=14)\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 60)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to make a best-fit line, what would you guess? Let's create a couple functions to make this easier to make a guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     8,
     24
    ]
   },
   "outputs": [],
   "source": [
    "# Plotting a guess of a regression line\n",
    "def regression_formula(x, a, b):\n",
    "    return a*x + b\n",
    "\n",
    "def plot_data_and_guess(slope, intercept, ax, x1=x, x2=y, **kwargs):\n",
    "    '''\n",
    "    Plot our data and regression line on the given axis.\n",
    "\n",
    "    Arguments:\n",
    "        slope : float\n",
    "            Value for the slope the regression line.\n",
    "            \n",
    "        intercept : float\n",
    "            Value for the intercept the regression line.\n",
    "        \n",
    "        ax : Axes\n",
    "            Axis to plot data and regression line\n",
    "        \n",
    "        x1 : array-like\n",
    "            Values along the x-axis\n",
    "        \n",
    "        x2 : array-like\n",
    "            Values along the y-axis\n",
    "        \n",
    "    Returns:\n",
    "        fig : Figure\n",
    "\n",
    "        ax : Axes\n",
    "    '''\n",
    "    # Plot data and regression line\n",
    "    ax.scatter(x1, x2)\n",
    "    yhat = regression_formula(x1, slope, intercept)\n",
    "    ax.plot(x1, yhat, 'r-', **kwargs)\n",
    "    \n",
    "    # Embelishments\n",
    "    ax.set_title('Data Points to Model')\n",
    "    ax.set_xlabel('x', fontsize=14)\n",
    "    ax.set_ylabel('y', fontsize=14)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 60)\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what do you think the regression parameters are?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alvaro\\AppData\\Local\\Temp\\ipykernel_14952\\2643534192.py:33: UserWarning: color is redundantly defined by the 'color' keyword argument and the fmt string \"r-\" (-> color='r'). The keyword argument will take precedence.\n",
      "  ax.plot(x1, yhat, 'r-', **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Data Points to Model'}, xlabel='x', ylabel='y'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Our guess\n",
    "guess = {\n",
    "    'slope': 10,\n",
    "    'intercept': 0,\n",
    "    'color':'orange'\n",
    "}\n",
    "\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "plot_data_and_guess(**guess, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would be your next guess be? \n",
    "\n",
    "- How can we tell when our guess is \"better\"?\n",
    "- Could we formalize this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way we can know how well our guess or _model_ did is to compare the predicted values with the actual values. These are the _residuals_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this would give us the error for each data point:\n",
    "\n",
    "$$ r_i = \\hat{y}_i - y_i $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_residuals(x_values, y_values, slope, intercept):\n",
    "    '''Find the residulas for each data point'''\n",
    "    yhat = intercept + slope*x_values\n",
    "    errors = y_values - yhat\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can go further by having just one number to represent how faithful our model was to the actual y-values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leads us to the idea of the **mean squared error** or **MSE**. This is all the residuals squared and then averaged:\n",
    "\n",
    "$$ MSE = \\frac{1}{n} \\sum_{i}^{n} (\\hat{y}_i - y_i)^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(x_values, y_values, slope, intercept):\n",
    "    \n",
    "    resid_sq = calculate_residuals(x_values, y_values, slope, intercept)**2 \n",
    "\n",
    "    return sum(resid_sq) / len(x_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our guess from earlier\n",
    "slope = guess.get('slope')\n",
    "intercept = guess.get('intercept')\n",
    "\n",
    "mse(x, y, slope, intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The function we use to find how bad our model did in prediction is typically called the **loss function**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we found here is great! We can now compare different models with one another.\n",
    "\n",
    "If we made a few different guesses, we could make our predictions and then calculate from the _loss function_ how good or bad our model did! We will want to find the _smallest loss_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our model changes based on the different model _parameters_ (the coefficients $\\beta_i$ for linear regression). \n",
    "\n",
    "If we imagine all the different ways we can adjust these parameters $\\vec{\\theta}$ and measure how well the model performs with the loss or **cost function** $J(\\vec{\\theta})$, we can plot this as a surface in this multidimensional plane. See the image below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/gradientdescent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that the terms **loss function** and **cost function** are frequently used interchangeably. Sometimes they are the same function, but sometimes they differ by making changes in the cost to improve _training_ or _learning_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try creating the cost function's curve/surface for just one parameter (slope) using our earlier data example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = np.zeros((20, 2))\n",
    "# Find the MSE for different slope values\n",
    "for idx, val in enumerate(range(40, 60)):\n",
    "    table[idx, 0] = val\n",
    "    table[idx, 1] = mse(x, y, slope=val, intercept=0)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(table[:, 0], table[:, 1], '-')\n",
    "plt.xlabel(\"Slope Values\", fontsize=14)\n",
    "plt.ylabel(\"MSE\", fontsize=14)\n",
    "plt.title(\"MSE with changes to slope\", fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this graph, what is the optimal slope value?\n",
    "\n",
    "How could we extend this to find the best slope _and_ intercept combination?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better Way of Guessing: Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this probably all sounds great! We just need to find the minimum of the cost function!\n",
    "\n",
    "But there's some bad news; we don't usually know what the cost function (which can be complicated!) \"looks\" like without trying a whole lot of different parameters $\\vec{\\theta}$. We'd need an _infinite_ number of parameter combinations to know $J(\\vec{\\theta})$ completely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what can we do?\n",
    "\n",
    "Well, we can take one \"guess\" (set of  parameters) and then measure $J(\\vec{\\theta})$. Then we can adjust our guess/parameters in a \"good\" direction, \"down the hill\". This is the basic idea of gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Gradient descent** is an optimization procedure that uses the _gradient_ (a generalized notion of a derivative) of the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we find this \"better\" guess? Well, we need to find the best direction to move \"downhill\" the fastest. We can do this with a generalization of the derivative called the **gradient**:\n",
    "\n",
    "$$\\begin{align}\\\\\n",
    "    \\large -\\nabla J &= -\\sum_i \\dfrac{\\partial J}{\\partial \\theta_i}\\hat{\\theta_i} \\\\\n",
    "            &= -\\frac{\\partial J}{\\partial \\theta_1}\\hat{\\theta_1} + \\dots +  \\frac{\\partial J}{\\partial \\theta_n}\\hat{\\theta_n}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the multivariate case, the gradient tells us how the function is changing **in each dimension**. A large value of the derivative with respect to a particular variable means that the gradient will have a large component in the corresponding direction. Therefore, **the gradient will point in the direction of steepest increase**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent in Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Make a guess at where the function attains its minimum value\n",
    "- Calculate the gradient/derivative at that point\n",
    "- Use that value to decide how to make your next guess!\n",
    "\n",
    "Repeat until we get the [derivative](https://www.mathsisfun.com/calculus/derivative-plotter.html) as close as we like to 0.\n",
    "\n",
    "If we want to improve our guess at the minimum of our loss function, we'll move in the **opposite direction** of the gradient away from our last guess. Hence we are using the *gradient* of our loss function to *descend* to the minimum value of the relevant loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stepping Down a Hill: Step Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we now have the basic idea of gradient descent of \"going down a hill\" and hopefully it's obvious that the steeper the hill, the more we can adjust our parameters to get to \"bottom\" (optimal parameters) faster.\n",
    "\n",
    "But a big question is how big of a step do we take?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The amount we adjust our parameter is determined by our **step size**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our steps are _too big_, we risk skipping over the minimum value (optimal parameters).\n",
    "\n",
    "If our steps are _too small_, it might take us too long to reach the minimum value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![learning_rate](https://www.jeremyjordan.me/content/images/2018/02/Screen-Shot-2018-02-24-at-11.47.09-AM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an elegant solution: Make the size of your step **proportional to the value of the derivative at the point where you currently are in parameter space**! If we're very far from the minimum, then our values will be large, and so we therefore can safely take a large step; if we're close to the minimum, then our values will be small, and so we should therefore take a smaller step.\n",
    "\n",
    "I said the size of the step is proportional to the value of the derivative. The constant of proportionality is often called the **\"learning rate\"**. \n",
    "\n",
    "This page helps to explain the dangers of learning rates that are too large and too small: https://www.jeremyjordan.me/nn-learning-rate/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note there are other optimizations we can do for gradient descent that rely on adjusting our cost function or how we take steps or both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general algorithm looks like this:\n",
    "\n",
    "We'll make a guess, $\\vec{s}$, at where our loss function attains a minimum. If we're not happy with how close the value of the gradient there is to 0, then we'll make a new guess, and the new guess will be constructed as follows:\n",
    "\n",
    "$\\large\\vec{s}_{new} = \\vec{s}_{old} - \\alpha\\nabla f(\\vec{s}_{old})$,\n",
    "\n",
    "where $\\alpha$ is the learning rate.\n",
    "\n",
    "In the one-dimensional case, we'll have:\n",
    "\n",
    "$\\large x_{new} = x_{old} - \\alpha\\frac{df}{dx}|_{x_{old}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to our original example and implement gradient descent to find the optimal parameters (slope and intercept)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(x, y)\n",
    "\n",
    "ax.set_title('Data Points to Model')\n",
    "ax.set_xlabel('x', fontsize=14)\n",
    "ax.set_ylabel('y', fontsize=14)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to find the gradient for the cost function (2-dimensions: $a$ & $b$; slope & intercept):\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial b}\\frac{1}{n}\\Sigma(y_i - (b + ax_i))^2 = -\\frac{2}{n}\\Sigma (y_i-ax_i - b)$$\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial a}\\frac{1}{n}\\Sigma(y_i - (b + ax_i))^2 = -\\frac{2}{n}\\Sigma x_i (y_i-ax_i - b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's formalize this into a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_deriv(a, b, x_i, y_i, respect_to):\n",
    "    '''\n",
    "    Get the partial derivative for cost function with respect to slope (a) \n",
    "    or intercept (b).\n",
    "    '''\n",
    "    if respect_to == 'b': # intercept\n",
    "        return (y_i - (a * x_i + b))\n",
    "    elif respect_to == 'a': # slope\n",
    "        return (x_i * (y_i - (a * x_i + b)))\n",
    "    else:\n",
    "        print('Choose either respect_to: a or b ')\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe: In the code above we've left out both the factors of two and the averages!\n",
    "\n",
    "- We'll take care of the averages below, but this is easily done because **the derivative of a sum is equal to the sum of the derivatives**: $\\frac{d}{dx}[f(x) + g(x)] = \\frac{df}{dx} + \\frac{dg}{dx}$.\n",
    "\n",
    "- The factors of two won't make any difference to our goals. Very often the cost function associated with some modeling task will be something like MSE and so have a squared term, and so then when we differentiate it we'll gain a factor of two. Clearly, minimizing $f(\\beta)$ and minimizing $2f(\\beta)$ will yield the same optimal $\\beta$, and so it's often convenient to leave off the factor of two from the expression of the derivative and so minimize the **half mean squared error** function: $\\frac{1}{2}\\Sigma(y - \\hat{y})^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's define the step we take (amount we adjust the parameters by) using the gradient and learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_gradient(a, b, x, y, learning_rate):\n",
    "    db = 0\n",
    "    da = 0 \n",
    "    # For each data point, update the derivative for the slope & intercept\n",
    "    N = len(x)\n",
    "    for i in range(N):\n",
    "        \n",
    "        # Partial derivatives of loss/cost function with respect to b & a\n",
    "        # Here's where we're taking our averages. Notice that we're leaving\n",
    "        # off the factors of 2.\n",
    "        db +=  -(1/N) * partial_deriv(a, b, x[i], y[i], respect_to='b')\n",
    "        da +=  -(1/N) * partial_deriv(a, b, x[i], y[i], respect_to='a')\n",
    "        \n",
    "    # Adjust the slope & intercept by the gradient\n",
    "    new_b = b - (learning_rate * db)\n",
    "    new_a = a - (learning_rate * da)\n",
    "    \n",
    "    return (new_a, new_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out and keep track of our guesses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guesses = []\n",
    "alpha = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our guess\n",
    "guess = {\n",
    "    'slope': 60,\n",
    "    'intercept': 10\n",
    "}\n",
    "\n",
    "guesses.append(guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "plot_data_and_guess(**guess, ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = step_gradient(guess['slope'], guess['intercept'], x, y, learning_rate=alpha)\n",
    "step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse(x, y, guess['slope'], guess['intercept'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's update our guess and try again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our guess using the new step\n",
    "new_slope, new_intercept = step\n",
    "guess = {\n",
    "    'slope': new_slope,\n",
    "    'intercept': new_intercept\n",
    "}\n",
    "guesses.append(guess)\n",
    "\n",
    "# Getting adjusted parameters\n",
    "step = step_gradient(guess['slope'], guess['intercept'], x, y, learning_rate=alpha)\n",
    "display(step)\n",
    "display(mse(x, y, guess['slope'], guess['intercept']))\n",
    "\n",
    "# Plotting out our new parameters\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "plot_data_and_guess(**guess, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat this another 200 times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(200):\n",
    "    # Our guess using the new step\n",
    "    new_slope, new_intercept = step\n",
    "    guess = {\n",
    "        'slope': new_slope,\n",
    "        'intercept': new_intercept\n",
    "    }\n",
    "    guesses.append(guess)\n",
    "\n",
    "    # Getting adjusted parameters\n",
    "    step = step_gradient(guess['slope'], guess['intercept'], x, y, learning_rate=alpha)\n",
    "    #  Only display every 10\n",
    "    if (i % 10) == 0:\n",
    "        print(f'Step # {i}:')\n",
    "        print(f'Slope: {step[0]} Intercept: {step[1]}')\n",
    "        print_mse = (mse(x, y, guess['slope'], guess['intercept']))\n",
    "        print(f'MSE for Step #{i} = {print_mse}')\n",
    "        print('-'*30)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does our final result look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting out our new parameters\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "plot_data_and_guess(**guesses[-1], ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the MSE over the guesses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mses = [\n",
    "    mse(x, y, d['slope'], d['intercept']) for d in guesses\n",
    "]\n",
    "plt.plot(range(len(mses)), mses);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was what we had for 200 iterations. What could we do to improve or speed up this method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "TOC",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
